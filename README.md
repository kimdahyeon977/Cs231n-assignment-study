# :unicorn: Cs231n assignment 스터디 :unicorn:  
 김다현, 홍일도 , 김성훈, 권소현, 이주안
- - -
<h1>Intro</h1> 


- - -
 
**Cs231n study** 내용을 정리한 레포입니다.  
**팀스터디**로 진행한 내용은 assignment 수행이고 개인적으로 **학습한 내용**들은 이론정리입니다.

- - -
● Cs231n - 2017

가장 평이 좋은 2016년도 강의와 2017년도 강의중 스터디원들의 의견을 반영하여 자막본 정리가 잘되어있는 2017년도 강의를 선택하였습니다.

강의를 들으며 Computer Vision의 원리와 개발 방향등을 학습하는 것과 더불어 이론 내용을 토론하며 확인하였습니다.

과제는 CNN 구조를 밑바닥부터 손수 개발하는 역량을 기르는 것을 목표로 하였습니다. 후에 논문 구현 연습을 하는데 도움이 되었습니다.
- - -

● 스터디 방식

해당 스터디는 매주 과제1의 소단원을 수행하고 강의를 정리해오는 것으로 구성되어 있고, 1주 시간을 할애하여 각 파트별 학습을 진행할 예정입니다.

1. 스터디 만남 전까지 학습(약 1시간 분량의 강의를 듣고 이론 내용 정리) 완료합니다.
2. 학습한 이론 내용을 바탕으로 Assignment를 수행합니다.
2. 정해진 날짜에 엘리스 플랫폼에서 정해진 인원이 돌아가면서 화면 공유와 함께 Assignment 결과를 발표합니다.
3. 이론을 완벽하게 이해해야만 코드를 구현할 수 있어 이론 내용을 확실하게 이해하는 것을 목표로 하여 토론할 내용을 생각해옵니다.
4. 프로젝트 형식이라 주 1회 모임을 갖지만 각자 자신의 성과를 보여주고 질문을 해소하기 위하여 소규모 스터디도 즉흥적으로 모집될 수 있습니다.

  
- - -
● 스터디 계획  
<h4>★ 2022/01/24 ~ 2022/02/28)  </h4>

1. 매주 강의 1개 수강 및 이론 내용 정리
2. 이론 내용을 들으면서 토론할 만한 내용 한명씩 총 5개 리스트업
3. Assignment 수행 및 코드 공유

- - -
● 스터디 모임 시간 & 기한

매주 토요일 오전 10시  
  
- - -
● 목표

1.	Cs231n assignment 수행 완료
2.  Computer Vision에 대한 원론적인 이론 탐구
3.  시간이 남을시에는 최신 논문 내용 공유
- - -
|**주차**|**목표**|**학습 내용**|
|:-----:|:---:|:-------:|
|1  (2022/01/24)|🚩Cs231n Lec1 수강, 앞으로의 계획 및 일정 수립|- 정기 모임 지정, 대회 주제 확정, 코랩 환경설정 세팅 완료
|2  (2022/01/30)|🚩Cs231n Lec2 수강 , assignment1의 KNN 수행
|3  (2022/02/07)|🚩Cs231n Lec3 수강 , assignment1의 SVM 수행
|4  (2022/02/14)|🚩Cs231n Lec4 수강 , assignment1의 softmax 수행
|5  (2022/02/21))|🚩Cs231n Lec5 수강 , assignment1의 two-layer 수행
|5  (2022/02/28))|🚩Cs231n Lec6 수강 , assignment1의 bonus, higher level 수행
- - -
<h2>2주차</h2>
● 2022/01/24: 스터디 소요 시간 40분(디스 코드 40분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**- 사전 학습 내용**
1. 엘리스 플랫폼 딥러닝 강의
2. 모두를 위한 딥러닝 수강 

**- 전체적인 계획** : 스터디 일정에 맞게 과제 수행 및 이론 토론
**- 숙제** : Cs231n Lec2 수강 , assignment1의 KNN 수행

- - -
<h2>2주차</h2>
● 2022/01/30: 스터디 소요 시간 90분(디스 코드 90분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**-학습 내용**
1. Cs231n Lec1 이론 내용 토론
 (1) x는 5 x 3072, W는 3072 x 10의 크기를 갖는 행렬일때 score 행렬의 크기는 ?
 (2) - 1. 다음 계산식은 multi-classSVM을 사용한 Loss 계산식입니다. 빈곳(A~F)을 순서대로 채우세요.
      ===계산식====
       Li = A ( B ( C , D - E + F ) )

      ㄱ. sum  ㄴ.Sj   ㄷ. max  ㄹ. 1  ㅁ.0   ㅂ.Syi   
 (3) 하이퍼파라미터를 설정할 때 데이터셋은 어떻게 나누어야 바람직할까?
 (4) Validation set을 사용하는 이유는?
 (5) 입력이 이미지인경우, KNN을 잘 사용하지 않는 이유 2가지는? 
 (6) L1 distance와 L2 distance의 특징을 예를 들어 서술하시오.
2. KNN 수행 및 코드 발표

- - -
<h2>3주차</h2>
● 2022/02/07: 스터디 소요 시간 60분(디스 코드 60분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**-학습 내용**
1. Cs231n Lec2 이론 내용 토론
 (1) SVM 디버깅 전략에 대해서 상세하게 설명해보기 (원리)
 (2) softmax가 지수를 사용하는 이유는?
 (3) SVM에서 loss가 가질 수 있는 최대 최소값은?
 (4) Validation set을 사용하는 이유는?
 (5) 일반적인 vanilla gradient descent 보다 SGD 가 선호되는 이유는 무엇인가?
 (6) SGD에 이용되는 기법으로 특정 데이터를 샘플링한 것을 무엇이라 하는가?
 (7) Numerical Gradient와 Analytic Gradient의 관계 및 각각의 장단점이 무엇인가?
2. SVM 수행 및 코드 발표
- - -
<h2>3주차</h2>
● 2022/02/07: 스터디 소요 시간 60분(디스 코드 60분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**-학습 내용**
1. Cs231n Lec3 이론 내용 토론
 (1) SVM 디버깅 전략에 대해서 상세하게 설명해보기 (원리)
 (2) softmax가 지수를 사용하는 이유는?
 (3) SVM에서 loss가 가질 수 있는 최대 최소값은?
 (4) Validation set을 사용하는 이유는?
 (5) 일반적인 vanilla gradient descent 보다 SGD 가 선호되는 이유는 무엇인가?
 (6) SGD에 이용되는 기법으로 특정 데이터를 샘플링한 것을 무엇이라 하는가?
 (7) Numerical Gradient와 Analytic Gradient의 관계 및 각각의 장단점이 무엇인가?
2. SVM 수행 및 코드 발표
- - -
<h2>4주차</h2>
● 2022/02/14: 스터디 소요 시간 40분(디스 코드 40분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**-학습 내용**
1. Cs231n Lec4 이론 내용 토론
 (1) ReLU를 사용하는 이유는?
 (2) Neural networks에서 생각나는 Activation function 2가지 
 (3) activation function 이란?
 (4) 연쇄 법칙 행렬구하기
2. Soft-max 수행 및 코드 발표
- - -
<h2>5주차</h2>
● 2022/02/21: 스터디 소요 시간 60분(디스 코드 60분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**-학습 내용**
1. Cs231n Lec5 이론 내용 토론
 (1) layer이 많으면 속도도 느리고 과적합도 심해지는데 왜 굳이 층을 여러개 만드는가?
 (2) pooling의 문제점은?
 (3) zero-padding과 같은 방법을 사용하는 이유는?
 (4)  특정 값에 대한 미분을 구하기 위해 다른 요소의 미분 값을 연결하여 구하는 방식을 무엇이라 하는가
2. two-layer 수행 및 코드 발표
- - -
<h2>6주차</h2>
● 2022/02/28: 스터디 소요 시간 50분(디스 코드 50분)

**- 언어, 개발환경** : python, google colab&kaggle notebook   
**-학습 내용**
1. Cs231n Lec6 이론 내용 토론
 (1) 이것은 그래디언트를 구하는 방식 중 모멘텀 방식을 개선한 것으로  이론적으로 모멘텀 방식 보다 항상 컨버전스 레이트가 좋다고 알려진 방식입니다. 
    비용 함수의 기울기를 계산할 때, 매개변수에 이전 모멘텀을 빼주어 다음 위치를 근사화하여 비용함수의 기울기를 계산합니다. 즉, 현재 파라미터로 기울기를 계산하는 것이 아니     라 파라미터의 다음 위치의 근사치로 기울기를 계산하는 것입니다. 다른 말로하면, 이전에 계산된 모멘텀항에서 다음 매개변수의 위치를 예측합니다.
    이 방식은 무엇일까요?
    
 (2) 7x7 입력, 3x3 필터 연산을 수행할 때 zero-padding을 하면 출력이 어떻게 될까?
2. bonus , higher level 수행 및 코드 발표
